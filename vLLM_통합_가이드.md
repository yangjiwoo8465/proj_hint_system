# vLLM í†µí•© ê°€ì´ë“œ

## ğŸ“š vLLM ê°œìš”

### vLLMì´ë€?
- **Very Large Language Model serving**ì˜ ì•½ì
- UC Berkeley LMSYS ì—°êµ¬íŒ€ ê°œë°œ
- **ê³ ì„±ëŠ¥ LLM ì¶”ë¡  ë° ì„œë¹™ ì—”ì§„**

### í•µì‹¬ ê¸°ìˆ 

#### 1. PagedAttention
```
ê¸°ì¡´ ë°©ì‹:              vLLM PagedAttention:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ KV Cache    â”‚        â”‚ P1â”‚ P2â”‚ P3â”‚ P4â”‚  í˜ì´ì§€ ë‹¨ìœ„ ê´€ë¦¬
â”‚ (ì—°ì† í• ë‹¹) â”‚        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚             â”‚        â”‚ P5â”‚   â”‚   â”‚   â”‚  í•„ìš”í•œ ë§Œí¼ë§Œ í• ë‹¹
â”‚ [ë‚­ë¹„ ë§ìŒ] â”‚        â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        [ë©”ëª¨ë¦¬ íš¨ìœ¨ 24ë°°â†‘]
```

- GPU ë©”ëª¨ë¦¬ë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ê´€ë¦¬
- KV ìºì‹œ ë™ì  í• ë‹¹ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë‚­ë¹„ ìµœì†Œí™”
- **ì²˜ë¦¬ëŸ‰ 24ë°° í–¥ìƒ**

#### 2. Continuous Batching
```
ê¸°ì¡´ Static Batching:
Batch 1: [Req1, Req2, Req3] â†’ ëª¨ë‘ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
         â–¼ GPU ìœ íœ´
Batch 2: [Req4, Req5, Req6]

vLLM Continuous Batching:
[Req1, Req2, Req3] â†’ Req2 ì™„ë£Œ ì¦‰ì‹œ Req4 íˆ¬ì…
        â–¼ GPU í•­ìƒ 100% í™œìš©
[Req1, Req4, Req3] â†’ Req1 ì™„ë£Œ ì¦‰ì‹œ Req5 íˆ¬ì…
```

#### 3. ì„±ëŠ¥ ë¹„êµ

| í•­ëª© | Hugging Face | vLLM | TensorRT-LLM |
|------|--------------|------|--------------|
| **ì²˜ë¦¬ëŸ‰** | 1x | **24x** | 10x |
| **ì§€ì—°ì‹œê°„** | ë†’ìŒ | **ë‚®ìŒ** | ë‚®ìŒ |
| **ë©”ëª¨ë¦¬ íš¨ìœ¨** | ë‚®ìŒ | **ë§¤ìš° ë†’ìŒ** | ë†’ìŒ |
| **êµ¬ì¶• ë‚œì´ë„** | ì‰¬ì›€ | **ì‰¬ì›€** | ì–´ë ¤ì›€ |
| **OpenAI API** | âœ— | **âœ“** | âœ— |

---

## ğŸš€ í”„ë¡œì íŠ¸ í†µí•© ë°©ë²•

### 1. vLLM Docker Compose ì„¤ì •

**íŒŒì¼: `docker-compose.vllm.yml`**

```yaml
version: '3.8'

services:
  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: hint_system_vllm
    ports:
      - "8001:8000"
    volumes:
      - ./models:/root/.cache/huggingface  # ëª¨ë¸ ìºì‹œ
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: >
      --model ModelCloud/Brumby-14B-Base-GPTQMODEL-W4A16-v2
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 1
      --gpu-memory-utilization 0.9
      --max-model-len 4096
      --trust-remote-code
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - hint_system_network
    restart: unless-stopped

networks:
  hint_system_network:
    external: true
```

### 2. Backend AI ì„¤ì • ëª¨ë¸ ì—…ë°ì´íŠ¸

**íŒŒì¼: `backend/apps/coding_test/models.py`**

```python
class AIModelConfig(models.Model):
    """AI ëª¨ë¸ ì„¤ì •"""
    MODE_CHOICES = [
        ('api', 'API ë°©ì‹ (Hugging Face)'),
        ('vllm', 'vLLM ì„œë²„'),  # ì¶”ê°€
        ('local', 'ë¡œì»¬ ëª¨ë¸'),
    ]

    MODEL_CHOICES = [
        ('Qwen/Qwen2.5-Coder-32B-Instruct', 'Qwen 2.5 Coder 32B'),
        ('Qwen/Qwen2.5-Coder-7B-Instruct', 'Qwen 2.5 Coder 7B'),
        ('ModelCloud/Brumby-14B-Base-GPTQMODEL-W4A16-v2', 'Brumby 14B (Quantized)'),  # ì¶”ê°€
    ]

    mode = models.CharField(max_length=10, choices=MODE_CHOICES, default='api')
    model_name = models.CharField(max_length=200, default='Qwen/Qwen2.5-Coder-7B-Instruct')
    api_key = models.CharField(max_length=200, blank=True, null=True)
    vllm_url = models.CharField(max_length=200, default='http://vllm-server:8000', blank=True)  # ì¶”ê°€
    # ... ê¸°ì¡´ í•„ë“œ
```

### 3. vLLM API ì—°ë™

**íŒŒì¼: `backend/apps/coding_test/vllm_client.py` (ì‹ ê·œ ìƒì„±)**

```python
"""vLLM í´ë¼ì´ì–¸íŠ¸"""
import requests
from typing import Optional, Dict, Any

class vLLMClient:
    """vLLM OpenAI í˜¸í™˜ API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, base_url: str = "http://localhost:8001"):
        self.base_url = base_url.rstrip('/')
        self.chat_endpoint = f"{self.base_url}/v1/chat/completions"
        self.models_endpoint = f"{self.base_url}/v1/models"

    def check_health(self) -> bool:
        """ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(self.models_endpoint, timeout=5)
            return response.status_code == 200
        except:
            return False

    def generate_hint(
        self,
        prompt: str,
        model: str = "ModelCloud/Brumby-14B-Base-GPTQMODEL-W4A16-v2",
        max_tokens: int = 200,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> Optional[str]:
        """íŒíŠ¸ ìƒì„±"""
        try:
            payload = {
                "model": model,
                "messages": [
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì†Œí¬ë¼í…ŒìŠ¤ì‹ í•™ìŠµë²•ì„ ì‚¬ìš©í•˜ëŠ” ì½”ë”© êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stream": False
            }

            response = requests.post(
                self.chat_endpoint,
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content'].strip()
            else:
                print(f"vLLM API Error: {response.status_code} - {response.text}")
                return None

        except Exception as e:
            print(f"vLLM Client Error: {str(e)}")
            return None
```

### 4. íŒíŠ¸ API ì—…ë°ì´íŠ¸

**íŒŒì¼: `backend/apps/coding_test/hint_api.py`**

```python
from .vllm_client import vLLMClient

@api_view(['POST'])
@permission_classes([IsAuthenticated])
def request_hint(request):
    """íŒíŠ¸ ìš”ì²­ API"""
    # ... ê¸°ì¡´ ì½”ë“œ ...

    ai_config = AIModelConfig.get_config()

    # vLLM ëª¨ë“œ ì¶”ê°€
    if ai_config.mode == 'vllm':
        try:
            vllm_client = vLLMClient(base_url=ai_config.vllm_url)

            if not vllm_client.check_health():
                hint_response = "vLLM ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
            else:
                hint_response = vllm_client.generate_hint(
                    prompt=prompt,
                    model=ai_config.model_name,
                    max_tokens=200,
                    temperature=0.7
                )

                if not hint_response:
                    hint_response = "íŒíŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        except Exception as e:
            print(f'vLLM Error: {str(e)}')
            hint_response = "íŒíŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ìƒê°í•´ë³´ì„¸ìš”."

    # ... ê¸°ì¡´ API, local ëª¨ë“œ ì½”ë“œ ...
```

### 5. Frontend ëª¨ë¸ ì„ íƒ UI ì—…ë°ì´íŠ¸

**íŒŒì¼: `frontend/src/pages/AdminPanel/tabs/ModelsTab/index.jsx`**

```jsx
{/* ëª¨ë“œ ì„ íƒì— vLLM ì¶”ê°€ */}
<label className={`mode-option ${aiMode === 'vllm' ? 'selected' : ''}`}>
  <input
    type="radio"
    name="aiMode"
    value="vllm"
    checked={aiMode === 'vllm'}
    onChange={(e) => setAiMode(e.target.value)}
  />
  <div className="mode-content">
    <div className="mode-title">âš¡ vLLM ì„œë²„ ë°©ì‹</div>
    <div className="mode-description">
      â€¢ ê³ ì„±ëŠ¥ ì¶”ë¡  ì—”ì§„ (24ë°° ë¹ ë¥¸ ì²˜ë¦¬)
      <br/>â€¢ PagedAttentionìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ ê·¹ëŒ€í™”
      <br/>â€¢ OpenAI API í˜¸í™˜
      <br/>â€¢ ë³„ë„ vLLM ì„œë²„ í•„ìš”
    </div>
  </div>
</label>

{/* ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´ */}
<select value={modelName} onChange={(e) => setModelName(e.target.value)}>
  <option value="Qwen/Qwen2.5-Coder-32B-Instruct">Qwen 2.5 Coder 32B</option>
  <option value="Qwen/Qwen2.5-Coder-7B-Instruct">Qwen 2.5 Coder 7B</option>
  <option value="ModelCloud/Brumby-14B-Base-GPTQMODEL-W4A16-v2">
    Brumby 14B (Quantized)
  </option>
</select>
```

---

## ğŸ”§ ì„¤ì¹˜ ë° ì‹¤í–‰

### 1. vLLM ì„œë²„ ì‹œì‘

```bash
# GPUê°€ ìˆëŠ” ê²½ìš°
docker-compose -f docker-compose.yml -f docker-compose.vllm.yml up -d

# ë˜ëŠ” ì§ì ‘ ì„¤ì¹˜
pip install vllm

# vLLM ì„œë²„ ì‹¤í–‰
vllm serve ModelCloud/Brumby-14B-Base-GPTQMODEL-W4A16-v2 \
    --host 0.0.0.0 \
    --port 8001 \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.9
```

### 2. ì„œë²„ ìƒíƒœ í™•ì¸

```bash
# ëª¨ë¸ ëª©ë¡ í™•ì¸
curl http://localhost:8001/v1/models

# í…ŒìŠ¤íŠ¸ ìš”ì²­
curl http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ModelCloud/Brumby-14B-Base-GPTQMODEL-W4A16-v2",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ]
  }'
```

### 3. Django ì„¤ì • ì—…ë°ì´íŠ¸

```bash
# ë§ˆì´ê·¸ë ˆì´ì…˜
python manage.py makemigrations
python manage.py migrate

# ì„œë²„ ì¬ì‹œì‘
docker-compose restart backend
```

---

## ğŸ’¡ ëª¨ë¸ íŠ¹ì§• ë¹„êµ

### Qwen 2.5 Coder 32B
- **í¬ê¸°**: 32.76 GB
- **íŠ¹ì§•**: ê°€ì¥ ê°•ë ¥í•œ ì½”ë”© ëŠ¥ë ¥
- **ë‹¨ì **: ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ë†’ìŒ, ë¬´ë£Œ API ë¯¸ì§€ì›
- **ìš©ë„**: ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜, ìµœê³  í’ˆì§ˆ íŒíŠ¸

### Qwen 2.5 Coder 7B
- **í¬ê¸°**: 7.6 GB
- **íŠ¹ì§•**: ê· í˜•ì¡íŒ ì„±ëŠ¥/ì†ë„
- **ì¶”ì²œ**: vLLMìœ¼ë¡œ ì„œë¹™ ì‹œ ìµœì 
- **ìš©ë„**: ì¼ë°˜ì ì¸ ì½”ë”© íŒíŠ¸

### Brumby 14B (Quantized)
- **í¬ê¸°**: 14.77 GB (ì–‘ìí™”)
- **íŠ¹ì§•**:
  - GPTQ 4bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆì•½)
  - ë² ì´ìŠ¤ ëª¨ë¸ (Instruction íŠœë‹ í•„ìš” ê°€ëŠ¥)
  - ì¤‘ê°„ í¬ê¸°
- **ìš©ë„**: ë¦¬ì†ŒìŠ¤ ì œí•œ í™˜ê²½

---

## âš™ï¸ ì¶”ì²œ ì„¤ì •

### ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½
```yaml
ëª¨ë“œ: vLLM
ëª¨ë¸: Qwen/Qwen2.5-Coder-7B-Instruct
GPU: RTX 3090 (24GB) ì´ìƒ
```

### í”„ë¡œë•ì…˜ í™˜ê²½
```yaml
ëª¨ë“œ: vLLM
ëª¨ë¸: Qwen/Qwen2.5-Coder-7B-Instruct
GPU: A100 (40GB) ê¶Œì¥
ìŠ¤ì¼€ì¼ë§: ë‹¤ì¤‘ GPU + Tensor Parallelism
```

### ë¦¬ì†ŒìŠ¤ ì œí•œ í™˜ê²½
```yaml
ëª¨ë“œ: API
ëª¨ë¸: ì‘ì€ ëª¨ë¸ ë˜ëŠ” Fallback
ëŒ€ì•ˆ: Hugging Face Serverless API (ìœ ë£Œ)
```

---

## ğŸ¯ vLLM ì‚¬ìš© ì¥ì 

1. âœ… **24ë°° ë¹ ë¥¸ ì²˜ë¦¬** - ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬ ëŠ¥ë ¥ ê·¹ëŒ€í™”
2. âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨** - PagedAttentionìœ¼ë¡œ ë” ë§ì€ ìš”ì²­ ì²˜ë¦¬
3. âœ… **OpenAI API í˜¸í™˜** - ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš© ê°€ëŠ¥
4. âœ… **ì‰¬ìš´ ë°°í¬** - Docker ì»¨í…Œì´ë„ˆë¡œ ê°„ë‹¨ ì„¤ì¹˜
5. âœ… **ì‹¤ì‹œê°„ ì‘ë‹µ** - Continuous Batchingìœ¼ë¡œ ì§€ì—° ìµœì†Œí™”

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **GPU í•„ìˆ˜**: vLLMì€ GPU í•„ìš” (CUDA ì§€ì›)
2. **ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰**: ëª¨ë¸ í¬ê¸° + ì—¬ìœ  ë©”ëª¨ë¦¬ í•„ìš”
3. **ì²« ì‹¤í–‰ ì‹œê°„**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë”©ì— ì‹œê°„ ì†Œìš”
4. **ì–‘ìí™” ëª¨ë¸**: BrumbyëŠ” GPTQ ì–‘ìí™”ë¡œ ì •í™•ë„ ì•½ê°„ í•˜ë½ ê°€ëŠ¥

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai)
- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [PagedAttention ë…¼ë¬¸](https://arxiv.org/abs/2309.06180)
- [Brumby ëª¨ë¸ ì¹´ë“œ](https://huggingface.co/ModelCloud/Brumby-14B-Base-GPTQMODEL-W4A16-v2)
